{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mediterranean-satellite",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "In this notebook we will train a Deep Deterministic Policy Gradient (DDPG) Agent to control a double-jointed arm to move and reach target locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from ddpg.agent import Agent\n",
    "from unityagents import UnityEnvironment\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-adapter",
   "metadata": {},
   "source": [
    "### Check for GPU device. If not available use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-marina",
   "metadata": {},
   "source": [
    "### Create the environment and set the default for controlling\n",
    "\n",
    "Please change the file name of the environment in case the file path or name is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Reacher_Linux/Reacher.x86_64\")\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-covering",
   "metadata": {},
   "source": [
    "### Check the environment for Action and State size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-camcorder",
   "metadata": {},
   "source": [
    "### Create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    'seed': 0,\n",
    "    'device': device,\n",
    "    'lr_actor': 1e-4,\n",
    "    'lr_critic': 1e-3,\n",
    "    'buffer_size': 100000,\n",
    "    'batch_size': 128,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 1e-3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, config=configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-dictionary",
   "metadata": {},
   "source": [
    "#### Helper method to train the agent on the environment\n",
    "**Parameters:**\n",
    "\n",
    "- **n_episodes(int)**: Maximum number of episodes to run the training process\n",
    "- **max_t(int)**: Maximum number of time step the agent will spend in each episode\n",
    "- **eps_start(float)**: Initial value of epsilon\n",
    "- **eps_end(float)**: Minimum value of epsilon\n",
    "- **eps_decay(float)**: The decay rate of epsilon for each episode\n",
    "- **max_score(float)**: The avg. score in last 100 episodes after which the environment will be considered as solved\n",
    "- **model_path(str)**: Path to save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, max_score=30.0, model_path='model.pt'):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps_end, eps*eps_decay)\n",
    "        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window)}\", end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window)}\")\n",
    "        if np.mean(scores_window) >= max_score:\n",
    "            print(f\"\\nEnvironment solved in {i_episode-100} episodes!\\tAverage Score: {np.mean(scores_window)}\")\n",
    "            torch.save(agent.actor_local.state_dict(), model_path)\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-bench",
   "metadata": {},
   "source": [
    "### Helper method to visualize the training scores\n",
    "**Parameters:**\n",
    "\n",
    "- **scores(list of floats)**: The scores collected for all episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(scores):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Scores')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-turkey",
   "metadata": {},
   "source": [
    "### Helper method to test a trained agent on the environment\n",
    "**Parameters**\n",
    "\n",
    "- **env**: The environmnts\n",
    "- **agent**: The agent object\n",
    "- **n_episodes(int)**: Number of episodes to run the test\n",
    "- **model_path**: Path to a saved pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(env, agent, n_episodes=3, model_path='model.pt'):\n",
    "    agent.actor_local.load_state_dict(torch.load(model_path))\n",
    "    total_score = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        score = 0\n",
    "        env_info = env.reset(train_mode=False)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        print(f\"\\rEpisode {i_episode} \\tScore: {score}\")\n",
    "        total_score += score\n",
    "    print(f\"\\rAverage score in {n_episodes} episodes: {total_score/n_episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-treasurer",
   "metadata": {},
   "source": [
    "### Run the training process\n",
    "\n",
    "Start training the agent and visualize the scores after the completion of the training. In case of any error close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scores = train()\n",
    "    visualize(scores)\n",
    "except Exception as e:\n",
    "    env.close()\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-manner",
   "metadata": {},
   "source": [
    "### Test the agent\n",
    "Use the trained agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(env=env, agent=agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-roller",
   "metadata": {},
   "source": [
    "### Close the environment after everything is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
